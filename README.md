# Research Papers

This repository contains `research papers` related to robotics.

[1] [Local_INN: Implicit Map Representation and Localization with Invertible Neural Networks](https://ieeexplore.ieee.org/document/10161015)

`Authors`: Zirui Zang; Hongrui Zheng; Johannes Betz; Rahul Mangharam

`DOI`: 10.1109/ICRA48891.2023.10161015

`Summary`: <div align="justify"> Use an INN network to map the robot's poses and the LiDAR scans. After training, network gives out an estimate of the distribution of the robot's poses for a given LiDAR scan information. To make sure the inputs and the outputs are of same dimensions in the network, `positional encoding` is used for robot's poses (to ↑ dimensions) and `VAE` is used for LiDAR scans (to ↓ dimensions). This distribution of estimated poses can be used in filtering techniques for improving the state estimation. The network also needs information of the previous state along with the LiDAR data to output the robot's poses distribution. Likewise, during training, we need the previous state information to map the robot's pose and the LiDAR scans. </div>

---

[2] [iMAP: Implicit Mapping and Positioning in Real-Time](https://ieeexplore.ieee.org/document/9710431)

`Authors`: Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew Davison

`DOI`: 10.1109/ICCV48922.2021.00617

`Summary`: <div align="justify"> iMAP presents a `dense SLAM` method that works in real-time through continual learning of an `MLP` trained on the images taken by an RGB-D camera. Here, only the `MLP` is used in the contruction of the map / scene in realtime. This paper presents a mechanism that runs both `tracking` and `mapping` simultaneouly using the `MLP`, but at different rates. In the pipeline, when a new RGB-D image is given, an optimization is run along with `MLP` to estimate the best possible camera pose. This is the `tracking` phase. The same picture is used (if it satisfies keyframe criteria) to train the `MLP` network for improved accuracy in the `mapping` phase. To achieve real-time implementation, instead of using all the pixels for training the MLP / NeRF, only a select group of pixels are used via `Active Sampling` technique. A similar stratergy is used in choosing the keyframes for training the `MLP`. </div>

![iMAP](https://github.com/saichand44/research-papers/assets/14955987/d518945f-35e2-402b-8652-227bb5d10224)

---

[3] [Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation](https://link.springer.com/chapter/10.1007/978-3-031-43996-4_51)

`Authors`: Dimitrios Psychogyios, Francisco Vasconcelos, Danail Stoyanov

`DOI`: https://doi.org/10.1007/978-3-031-43996-4_51

`Summary`: <div align="justify"> This paper presents a method to generate synthetic data using a given set of images. The artificial data is generated by querying a NeRF which essentially creates novel views pertaining to an unknown camera pose not seen in the training data. Here NeRF is conditioned on the `location and orientation of the light source` and is supervised using `depth map` during training (to account for lack of hemispherical distribution of camera and lack of texture along inner walls). The loss function for training the NeRF includes both depth map and color map. </div>

![Light Conditioned NeRF](https://github.com/saichand44/research-papers/assets/14955987/c406b507-9590-4821-b2b4-89aec8ef27a0)

---

[4] [MAGIC-VFM: Meta-learning Adaptation for Ground Interaction Control with Visual Foundation Models](https://www.researchgate.net/publication/382331849_MAGIC-VFM_Meta-learning_Adaptation_for_Ground_Interaction_Control_with_Visual_Foundation_Models)

`Authors`: Elena Sorina Lupu, Fengze Xie, James A. Preiss, Jedidiah Alindogan, Matthew Anderson, Soon-Jo Chung

`DOI`: https://doi.org/10.48550/arXiv.2407.12304

`Summary`: <div align="justify"> This paper presents a `learning-based adaptive control` that improves the tracking error between the actual and reference trajectory. For an uncertain dynamical system, where we cannot fully capture the dynamics of the system, we consider a process noise or disturbance that accounts for the discrepencies. This `noise / disturbance` can depend on several factors including the environment the robot is operating. The idea is to estimate this noise better by identifying the terrain the robot is operating and hence it uses a camera to capture the terrain, which then is fed to a `VFM (Dino V1)` to convert it to a feature vector. This papers presents a method to feed an adaptive controller a comprehensive information about the dynamics by considering the `terrain (from VFM)`, `robot's state` for a better input control that ensures minimal tracking error. Precisely, in the process dynamics, this paper aims to model the noise as a `DNN`. For the training purpose, the `DNN` uses the `robot states`, `feature vectors related to the terrain from VFM` as inputs and `difference of actual and predicted dynamics` as labels (for the noise). The `DNN` also has online adaptation to account for changes during operation. It discusses the simulation / hardware results of the new control on varying terrain conditions and how it compensates for the `longitudinal slip` on different terrains. Additionally, the paper discusses the efficacy of the adaptive control for wear and tear of the drive mechanism (track / wheels). </div>

![Magic-VFM Workflow](https://github.com/user-attachments/assets/31973e4b-a9e2-46c2-a109-63061b066a3f)

---

[5] [DiffStack: A Differentiable and Modular Control Stack for Autonomous Vehicles](https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DteEnA3L4aRe&sa=D&sntz=1&usg=AOvVaw3xfB455sSquEJWv2lRzMgr)

`Authors`: Peter Karkus, Boris Ivanovic, Shie Mannor, Marco Pavone

`DOI`: [https://doi.org/10.1007/978-3-031-43996-4_51](https://doi.org/10.48550/arXiv.2212.06437)

`Summary`: <div align="justify"> This paper presents a differentiable and modular `Autonomous Vehicle` stack for prediction, planning and control and enables a way for `planning aware prediction`. Traditionally, AV stacks are independent and modular with explicit components performing perception, prediction, planning and control, but these suffer from compounding errors and integration challenges. At the other end, there's end-to-end neural network approach that can handle integration issues, compounding errors since the final cost is dependent on the entire network, but this suffers from modularity. `DiffStack` is the best of both worlds where modular stacks are trained with their respective training losses. This papers mainly discusses this approach in the context of `prediction` (of non-AV agents), `planning` (of ego car) and `control` (of ego car). The NN approach enables the `prediction` module to consider the final control objective. The training and tests are performed on [**nuScenes**](https://www.nuscenes.org/) dataset. Overall, `DiffStack` a) makes fewer prediction erroes that effect planning, b) accounts for system integration erros</div>

![DiffStack1](https://github.com/user-attachments/assets/7f8d686a-34ad-4f4f-bc7a-7c5d8f1e08a4)
![DiffStack2](https://github.com/user-attachments/assets/a0c1befc-284c-4675-bd27-1d92936bb2ad)

---
